{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab #1 - Sandbox notebook for the use case: Courier default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we are all data-scientists, and we are currently working on the exploration and development of our models for solving the courier default use case.\n",
    "\n",
    "-------\n",
    "\n",
    "**Courier default description:**\n",
    "\n",
    "*\"We define courier default when a courier collects a high amount of cash and then leaves the company without returning the money.\n",
    "\n",
    "- By leaving the company we define as not having any interaction with us during 28 consecutive days since the last delivered order.\n",
    "- Each region/city/country has a threshold to designate a high amount of cash defined by the business.\n",
    "\n",
    "The business request made to the Data Science team is to design a machine learning system to detect when a courier is likely to incur in default before they actually default or before the amount of cash balance is too high.\n",
    "\n",
    "The proposed definition is to deliver a default score for each courier every week. For that we created a label for each courier on a window time, so we can identify some defaulters and then fit a xgboost classifier to predict on current couriers.\"*\n",
    "\n",
    "-------\n",
    "\n",
    "In this notebook we will explore different methods for working with this binary classification use case based on XGBoost, in order to predict the default or not default condition for the couriers.\n",
    "\n",
    "Let's try with these approaches:\n",
    "\n",
    "1. We will load the existing XGBoost model file (trained outside of AWS), and analyse it with open-source XGBoost.\n",
    "2. We will repeat the analysis levaraging on Amazon SageMaker Autopilot for using AutoML.\n",
    "3. Finally, we will repeat the step 1 but leveraging on Amazon SageMaker built-in XGBoost algorithm, and we will use the hyperparameters from step 2.\n",
    "\n",
    "#### Importing relevant packages...\n",
    "\n",
    "Let's start by loading some relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import datetime as dt\n",
    "import pickle as pkl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Working with the existing model\n",
    "\n",
    "Let's work with the dataset provided, and perform the same feature selection done as per the use case description.\n",
    "\n",
    "**IMPORTANT:**\n",
    "Make sure you get the dataset and the model files from the workshop organizers (joint_dataframe.csv & model-sav), and upload it to your Jupyter environment by choosing \"Upload\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "\n",
    "import xgboost as xgb\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('joint_dataframe.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store the target feature in a separate dataframe\n",
    "\n",
    "y_data = data['is_defaulter']\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop some features as per the use case description\n",
    "\n",
    "cols = [0, 1, 2, 3, 4, 5, 6, 8, 9, 56, 191, 201, 203, 206, 209, 218, 220]\n",
    "data.drop(data.columns[cols],axis=1,inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset in train/test, and convert the dataframes to XGBoost matrixes\n",
    "\n",
    "X = data.values\n",
    "Y = y_data.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the existing model trained outside AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load existing model\n",
    "\n",
    "model = pkl.load(open(\"model-sav\", \"rb\"))\n",
    "type(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run predictions over the testing dataset, and evaluate some model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to have decent accuracy but the precision and recall seems to be low.\n",
    "\n",
    "Let's confirm the confusion matrix to understand how much false negatives and false positives we are having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, (preds>0.5))\n",
    "print('The Confusion Matrix is: \\n', cm)\n",
    "print('TrueNegative   FalsePositive\\nFalseNegative   TruePositive\\n')\n",
    "\n",
    "# Calculate the accuracy on test set\n",
    "predict_accuracy_on_test_set = (cm[0,0] + cm[1,1])/(cm[0,0] + cm[1,1]+cm[1,0] + cm[0,1])\n",
    "print('The Accuracy on Test Set is: ', predict_accuracy_on_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices where is_defaulter should be one: \", np.nonzero(Y_test > 0))\n",
    "print(\"\\nIndices where is_defaulter is one: \", np.nonzero(preds > 0))\n",
    "\n",
    "print(\"\\n------- False negative case: -------\")\n",
    "print(\"Defaulter data: \\n\", list(X_test[3][:]))\n",
    "print(\"The true is_defaulter value: \", Y_test[3], \" the predicted is_defaulter value: \", preds[3])\n",
    "sample = [60.11, 110.0, 0.99, 0.09, 5.0, 0.77, 0.44, 0.5, 0.36, 2.29, 0.01, 48.0, 0.02, 1.0, 110.0, 0.64, 1.01, 0.71, 0.00495662949194547, 4.0, 0.02, 0.98, 807.0, 1.0, 4.0, 111.0, 1.33, 0.44, 0.0, 5.0, 11.0, 0.5128205128205128, 0.0, 39.0, 2.0, 0.18181818181818185, 0.0924170616113744, 0.0196078431372549, 0.07471264367816091, 39.0, 1.0, 0.0, 0.0, 0.8478260869565217, 0.96, 0.38, 31.43, 0.05, 62.46, 3.21252782376189, 60.11, 2.0, 0.7, 37.0, 1.297491039426523, 47.0, 213.0, 38.0, 753.0, 1.027027027027027, 181.0, 0.7926315789473685, 0.5046480743691899, 1.0734463276836157, 0.5498007968127491, 147.0, 0.668086239020495, 414.0, 0.694555112881806, 1.2312925170068028, 523.0, 0.8497652582159625, 10.02, 0.46, 10.43, 1172.27, 189.4, 0.0, 2.3, 0.0, 2196.57, 1.0, 0.0, 0.04, 211355.43, 84.48, 1.91, 0.0, 59.3, 0.0, 1.03, 0.7, 0.01, 0.02, 298.17, 0.689604842322592, 0.0, 227.86, 0.0, 52.07, 1.94, 0.02, 0.9, 395913.79, 0.31, 0.0, 1.28, 12.71, 83.74, 765.42, 0.1, 579.95, 0.01, 1.0, 8604.43, 130.06, 0.54, 70.1718644837339, 0.0, 0.0, 0.0, 0.56, 24.0, 9.0, 0.66, 0.0, 0.0, 19.0, 127.230466482265, 85.90011414995679, 55.0, 0.16, 1.07, 0.0, 71.6590049396513, 0.26, 0.0, 42.08, 0.0, 0.0, 37.45, 0.0, 1.08, 0.0, 10.0, 37.96, 90.0, 0.0, 19.0, 0.0, 55.0, 0.5, 28.0, 0.37, 0.0, 0.31, 40.0, 90.0, 0.16, 40.45, 38.0, 27.0, 173.0, 0.0, 178.0, 0.41, 0.24, 0.0, 0.49, 0.0, 0.0, 0.51, 46.57, 28.0, 0.32, 0.0, 0.0, 90.8904313776849, 0.52, 0.93, 1.0, 7.0, 0.0, 36.9236331328595, 0.0, 825.0, 0.0, 1.0, 1.0, 0.0, 1.53846153846154, 26.23, 0.0, 2.4935483870967703, 0.0, 6.125, 27.0, 753.0, 7.0, 83.0, 0.0, 1.5609320122090802, 21.6833, 310.0, 0.46524609707560494, 143.92, 13.0, 0.0, 0.91, 0.24, 21.0, 380.0, 1.65, 1.77, 80.0, 5.0, 41.0, 0.45, 0.45, 36.0, 0.89, 57.4, 0.41, 0.57, 0.1192, 0.94113029827315, 232.33333333333331, 23.98, 0.0784, 10.5, 11.08, 0.8849926199200001, 0.78, 28.5, 6.97, 27.37, 27.2, 0.0491, 0.39, 0.1769, 2.87, 0.022370486656200003, 0.4, 0.73, 0.57, 24.33333333333333, 28.0, 0.15, 0.71, 12.0, 18.0, 12.0, 2.33, 9.0, 0.13, 0.74, 100000.0, 0.0, 1.77, 0.13, 0.22, 0.0, 0.3193277310924369, 950.0, 0.0, 0.2689075630252101, 0.0, 0.9, 0.22, 10.0, 0.56, 0.6956, 10000.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5384, 689.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.54, 0.0, 10000.0, 0.0, 4.0, 1.54, 1.0, 0.8571, 0.0, 1.0, 0.0, 1.4, 1.54, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2, 0.0, 0.0, 1.0, 360.0, 24.0, 0.8343309106455751, 288.0, 48.0, 0.8379373848987108]\n",
    "pred = model.predict(sample)\n",
    "print(pred)\n",
    "\n",
    "print(\"\\n------- False positive case: -------\")\n",
    "print(\"Defaulter data: \\n\", list(X_test[1886][:]))\n",
    "print(\"The true is_defaulter value: \", Y_test[1886], \" the predicted is_defaulter value: \", preds[1886])\n",
    "sample = [184.0, 2900.0, 0.98, 0.39, 5.0, 0.0, 0.51, 0.5, 4.21, 1.78, 0.02, 23.0, 0.12, 71.0, 40.85, 0.65, 1.02, 0.0, 0.00825206301575393, 4.0, 0.04, 0.96, 3999.0, 1.0, 33.0, 2971.0, 0.0, 0.58, 7.142857142857142, 1.0, 3.0, 6.7857142857142865, 5.0, 180.0, 19.0, 6.333333333333332, 0.03443657929978955, 0.04231625835189309, 0.030349013657056136, 127.0, 0.7055555555555556, 5.0, 0.04273504273504273, 6.143344709897611, 0.07, 1.89, 145.4, 0.77, 2599.0, 1038.0, 184.0, 12.0, 0.49, 20.0, 4.235171696149844, 110.0, 443.0, 99.0, 5422.0, 4.95, 407.0, 0.9366039039557782, 0.2816303946883069, 3.928571428571429, 0.3865732202139432, 132.0, 7.147376746638543, 2096.0, 0.6099225378089266, 3.083333333333333, 3307.0, 0.9187358916478556, 7.12, 4.54, 17.06, 4781.02, 438.94, 0.0, 1.45, 0.0, 22482.46, 1.0, 0.89, 0.0, 263691.34, 277.56, 0.92, 0.0, 339.35, 3.9586415365578294e-05, 3.4, 1.22, 0.0, 0.0, 1015.04, 0.0, 0.0, 379.52, 0.0, 0.0, 0.32, 0.0, 7.12, 1077023.92, 0.77, 0.0, 3.31, 0.0, 1225.2, 0.0, 0.02, 436.29, 0.0, 0.0, 95907.23, 74.76, 0.59, 41.322943430657, np.nan, 0.0, np.nan, 0.72, np.nan, np.nan, 0.82, 0.0, 0.0, np.nan, 60.1107199324761, 50.785041042695, np.nan, 0.0, 1.42, np.nan, 48.1914191364083, 0.0, 0.0, 32.95, 0.0, 0.0, 25.9, 0.0, 0.89, 0.0, np.nan, 24.37, np.nan, 0.0, np.nan, np.nan, np.nan, 0.0, np.nan, 0.57, np.nan, 0.0, np.nan, np.nan, 0.0, 34.5, np.nan, np.nan, np.nan, np.nan, np.nan, 0.52, 0.0, np.nan, 0.65, 0.0, np.nan, 0.0, 34.03, np.nan, 0.0, 0.0, 0.0, 49.9810437910087, 0.0, 1.05, 0.0, 1.0, 0.0, 100.0, 0.0, 5622.0, 0.0, 1.0, 1.0, 3.0, 1.35, 18.46, 0.0, 10.0, 0.0, 35.0, 50.0, 5422.0, 1.0, 10.0, 0.0, 1.2351362605839098, 35.3166, 9.0, 1.0377370041887801, 4.94, 83.0, 20.0, 4.65, 0.0, 0.0, 1527.0, 0.0, 0.0, 0.0, 37.0, 12.0, 4.44, 0.0, 136.0, 6.45, 18.15, 0.52, 0.35, 1.005, 0.86319050077439, 234.0, 16.72, 0.5118, 12.99, 12.68, 1.1144399999999999, 0.18, 16.6, 7.02, 21.89, 17.81, 0.6659999999999999, 0.73, 0.602, 3.63, 0.018069179143000002, 0.58, 4.6, 0.83, 153.33333333333334, 13.0, 0.02, 6.76, 0.0, 3.0, 196.0, 130000.0, 127.0, 0.03, 0.52, 30000.0, 0.0, 0.7, 0.03, 0.03, 0.0, 0.14545454545454545, 5789.0, 0.0, 0.2545454545454545, 0.0, 0.0, 0.1, 3.0, 0.6521, 0.8571, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 10000.0, 1.0, 1.0, 10000.0, 0.84, 0.0, 1.0, 0.0, 1.0, 1.5384, 3909.0, 33.33, 1.0, 0.0, 8.33, 0.0, 0.0, 5.56, 0.0, 0.0, 7.0, 0.0, 0.0, 1.1538, 0.0, 2.0, 10000.0, 1.5, 4.88, 0.0, 1.0, 0.14, 0.0, 0.0, 0.0, 1.2857, 0.0, 10000.0, 0.0, 2092.0, 141.0, 0.9225806451612903, 4995.0, 126.0, 0.41353248155479577]\n",
    "pred = model.predict(sample)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Now, let's try with Amazon SageMaker built-in XGBoost, but with the same feature selection as per Glovo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen a method for working with the example use case provided. Now, let's explore how could we replicate the case using the Amazon SageMaker built-in XGBoost algorithm.\n",
    "\n",
    "In this case we will just perform the same feature selection as per the previous case, but we will consider some hyperparameters adjustments. It should result on just a slightly better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set some parameters for starting\n",
    "\n",
    "print('Amazon SageMaker version:', sagemaker.__version__)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "print('Using region:', region,'\\nUsing role:',role)\n",
    "\n",
    "###########################################################\n",
    "##### REPLACE with the bucket name for your S3 bucket #####\n",
    "###########################################################\n",
    "bucket = 'YOUR BUCKET NAME GOES HERE'\n",
    "prefix = 'glovo/sandbox/builtin'\n",
    "\n",
    "print('Using bucket and prefix: {}/{}'.format(bucket, prefix))\n",
    "\n",
    "data = pd.read_csv('joint_dataframe.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0, 1, 2, 3, 4, 5, 6, 8, 56, 191, 201, 203, 206, 209, 218, 220]\n",
    "data.drop(data.columns[cols],axis=1,inplace=True)\n",
    "#data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = pd.concat([data['is_defaulter'], data.drop(['is_defaulter'], axis=1)], axis=1) \n",
    "\n",
    "train_data, validation_data, test_data = np.split(data.sample(frac=1, random_state=1729), [int(0.7 * len(data)), int(0.95 * len(data))])\n",
    "train_data.to_csv('train-built.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation-built.csv', header=False, index=False)\n",
    "test_data.to_csv('test_real-built.csv', header=False, index=False)\n",
    "test_data.drop(['is_defaulter'], axis=1).to_csv('test-built.csv', header=False, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train-built.csv')).upload_file('train-built.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation-built.csv')).upload_file('validation-built.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test-built.csv')).upload_file('test-built.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the split data in S3 and ready for training, we will setup an Amazon SageMaker XGBoost training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\n",
    "\n",
    "xgbb = sagemaker.estimator.Estimator(containers[region],\n",
    "                                    role,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "xgbb.set_hyperparameters(objective='binary:logistic',\n",
    "                         alpha = 0.0017958870152480393,\n",
    "                         colsample_bytree = 0.8974444697232986,\n",
    "                         eta = 0.378416419404957,\n",
    "                         gamma = 0.0038479366336815115,\n",
    "                         max_depth = 22,\n",
    "                         min_child_weight = 3.4445863514152535,\n",
    "                         num_round = 139,\n",
    "                         subsample = 0.7432022124726009\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After around 5 minutes we are ready to deploy this model on an endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb_predictor = xgbb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After around 6 minutes we have an Endpoint ready to serve inferences.\n",
    "Let's run the test data inferences on the endpoint, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb_predictor.content_type = 'text/csv'\n",
    "xgbb_predictor.serializer = csv_serializer\n",
    "xgbb_predictor.deserializer = None\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgbb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_target = test_data.drop(test_data.columns[0],axis=1)\n",
    "#test_data_no_target.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsb = predict(test_data_no_target.values)\n",
    "predsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testb = test_data.values[:,0]\n",
    "print(Y_testb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "best_predsb = np.asarray([np.argmax(line) for line in predsb])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(Y_testb, best_predsb, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_testb, best_predsb, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_testb, best_predsb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmb = confusion_matrix(Y_testb, (predsb>0.5))\n",
    "print('The Confusion Matrix is: \\n', cmb)\n",
    "print('TrueNegative   FalsePositive\\nFalseNegative   TruePositive\\n')\n",
    "\n",
    "# Calculate the accuracy on test set\n",
    "predict_accuracy_on_test_setb = (cmb[0,0] + cmb[1,1])/(cmb[0,0] + cmb[1,1]+cmb[1,0] + cmb[0,1])\n",
    "print('The Accuracy on Test Set is: ', predict_accuracy_on_test_setb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this model is a bit better than our first case, but still has a lot of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finally, let's try with Amazon SageMaker Autopilot\n",
    "\n",
    "Now, we will repeat the work but this time leveraging on AutoML. Obviously we expect better results as the previous examples were just doing a feature selection, while the AutoML should perform a full feature engineering and hyper-parameters optimization.\n",
    "\n",
    "*For more info on Amazon SageMaker Autopilot for AutoML you can check the documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html), and use [this example notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/autopilot/autopilot_customer_churn_high_level_with_evaluation.ipynb) as a reference.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set some parameters for starting\n",
    "\n",
    "print('Amazon SageMaker version:', sagemaker.__version__)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "print('Using region:', region,'\\nUsing role:',role)\n",
    "\n",
    "prefix = 'glovo/sandbox'\n",
    "\n",
    "print('Using bucket and prefix: {}/{}'.format(bucket, prefix))\n",
    "\n",
    "df = pd.read_csv('joint_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare original data for AutoML\n",
    "## This time we won't perform any feature selection as this will be done by SageMaker Autopilot\n",
    "\n",
    "train_data = df.sample(frac=0.8,random_state=200)\n",
    "\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "test_data_no_target = test_data.drop(columns=['is_defaulter'])\n",
    "\n",
    "train_file = 'train_data.csv'\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "\n",
    "test_file = 'test_data.csv'\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup an Amazon SageMaker Autopilot job\n",
    "\n",
    "### Adjust the max_candidates parameter below, this will affect the time it takes to complete the AutoML job\n",
    "\n",
    "from sagemaker import AutoML\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "base_job_name = 'glovo-automl-sdk-' + timestamp_suffix\n",
    "\n",
    "target_attribute_name = 'is_defaulter'\n",
    "target_attribute_values = np.unique(train_data[target_attribute_name])\n",
    "target_attribute_true_value = target_attribute_values[1] # 'True.'\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "automl = AutoML(role=role,\n",
    "                target_attribute_name=target_attribute_name,\n",
    "                base_job_name=base_job_name,\n",
    "                sagemaker_session=session,\n",
    "                max_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's train this AutoML\n",
    "### Note this step should take several minutes to complete depending on the max_candidates above\n",
    "\n",
    "automl.fit(train_file, job_name=base_job_name, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('JobStatus - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "\n",
    "describe_response = automl.describe_auto_ml_job()\n",
    "print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = automl.describe_auto_ml_job()\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "    print(describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate Autopilot candidates looking for the best training job\n",
    "\n",
    "best_candidate = automl.describe_auto_ml_job()['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Autopilot candidates to evaluate and run batch transform jobs.\n",
    "# Do not put a larger TOP_N_CANDIDATES than the Batch Transform limit for ml.m5.xlarge instances in your account.\n",
    "TOP_N_CANDIDATES = 5\n",
    "\n",
    "candidates = automl.list_candidates(sort_by='FinalObjectiveMetricValue',\n",
    "                                    sort_order='Descending',\n",
    "                                    max_results=TOP_N_CANDIDATES)\n",
    "\n",
    "for candidate in candidates:\n",
    "    print(\"Candidate name: \", candidate['CandidateName'])\n",
    "    print(\"Objective metric name: \", candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "    print(\"Objective metric value: \", candidate['FinalAutoMLJobObjectiveMetric']['Value'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_transform = session.upload_data(path=test_file, bucket=bucket, key_prefix=prefix)\n",
    "print('Uploaded transform data to {}'.format(input_data_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_response_keys = ['predicted_label', 'probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform batch trasnformation\n",
    "\n",
    "s3_transform_output_path = 's3://{}/{}/inference-results/'.format(bucket, prefix);\n",
    "\n",
    "transformers = []\n",
    "\n",
    "for candidate in candidates:\n",
    "    model = automl.create_model(name=candidate['CandidateName'],\n",
    "                                candidate=candidate,\n",
    "                                inference_response_keys=inference_response_keys)\n",
    "    \n",
    "    output_path = s3_transform_output_path + candidate['CandidateName'] +'/'\n",
    "    \n",
    "    transformers.append(\n",
    "        model.transformer(instance_count=1, \n",
    "                          instance_type='ml.m5.xlarge',\n",
    "                          assemble_with='Line',\n",
    "                          output_path=output_path))\n",
    "\n",
    "print(\"Setting up {} Batch Transform Jobs in `transformers`\".format(len(transformers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer in transformers:\n",
    "    transformer.transform(data=input_data_transform, split_type='Line', content_type='text/csv', wait=False)\n",
    "    print(\"Starting transform job {}\".format(transformer._current_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_complete = True\n",
    "\n",
    "while pending_complete:\n",
    "    pending_complete = False\n",
    "    num_transform_jobs = len(transformers)\n",
    "    for transformer in transformers:\n",
    "        desc = sm.describe_transform_job(TransformJobName=transformer._current_job_name)\n",
    "        if desc['TransformJobStatus'] not in ['Failed', 'Completed']:\n",
    "            pending_complete = True\n",
    "        else:\n",
    "            num_transform_jobs -= 1\n",
    "    print(\"{} out of {} transform jobs are running.\".format(num_transform_jobs, len(transformers)))\n",
    "    sleep(30)\n",
    "    \n",
    "for transformer in transformers:\n",
    "    desc = sm.describe_transform_job(TransformJobName=transformer._current_job_name)\n",
    "    print(\"Transform job '{}' finished with status {}\".format(transformer._current_job_name, desc['TransformJobStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate batch inference results\n",
    "\n",
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:].strip('/')\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for transformer in transformers:\n",
    "    print(transformer.output_path)\n",
    "    pred_csv = get_csv_from_s3(transformer.output_path, '{}.out'.format(test_file))\n",
    "    predictions.append(pd.read_csv(io.StringIO(pred_csv), header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, classification_report, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = test_data[target_attribute_name].apply(lambda row: True if row==target_attribute_true_value else False)\n",
    "\n",
    "# calculate auc score\n",
    "for prediction, candidate in zip(predictions, candidates):\n",
    "    roc_auc = roc_auc_score(labels, prediction.loc[:,1])\n",
    "    ap = average_precision_score(labels, prediction.loc[:,1])\n",
    "    print('%s\\'s ROC AUC = %.2f, Average Precision = %.2f' % (candidate['CandidateName'], roc_auc, ap))\n",
    "    print(classification_report(test_data[target_attribute_name], prediction.loc[:,0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr = []\n",
    "for prediction in predictions:\n",
    "    fpr, tpr, _ = roc_curve(labels, prediction.loc[:,1])\n",
    "    fpr_tpr.append(fpr)\n",
    "    fpr_tpr.append(tpr)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor='w', edgecolor='k')\n",
    "plt.plot(*fpr_tpr)\n",
    "plt.legend([candidate['CandidateName'] for candidate in candidates], loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = []\n",
    "for prediction in predictions:\n",
    "    precision, recall, _ = precision_recall_curve(labels, prediction.loc[:,1])\n",
    "    precision_recall.append(recall)\n",
    "    precision_recall.append(precision)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor='w', edgecolor='k')\n",
    "plt.plot(*precision_recall)\n",
    "plt.legend([candidate['CandidateName'] for candidate in candidates], loc=\"lower left\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_min_precision = 0.75\n",
    "\n",
    "best_recall = 0\n",
    "best_candidate_idx = -1\n",
    "best_candidate_threshold = -1\n",
    "candidate_idx = 0\n",
    "for prediction in predictions:\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, prediction.loc[:,1])\n",
    "    threshold_idx = np.argmax(precision>=target_min_precision)\n",
    "    if recall[threshold_idx] > best_recall:\n",
    "        best_recall = recall[threshold_idx]\n",
    "        best_candidate_threshold = thresholds[threshold_idx]\n",
    "        best_candidate_idx = candidate_idx\n",
    "    candidate_idx += 1\n",
    "\n",
    "print(\"Best Candidate Name: {}\".format(candidates[best_candidate_idx]['CandidateName']))\n",
    "print(\"Best Candidate Threshold (Operation Point): {}\".format(best_candidate_threshold))\n",
    "print(\"Best Candidate Recall: {}\".format(best_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_default = predictions[best_candidate_idx].loc[:,0] == target_attribute_true_value\n",
    "prediction_updated = predictions[best_candidate_idx].loc[:,1] >= best_candidate_threshold\n",
    "\n",
    "# compare the updated predictions to Autopilot's default\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"Default Operating Point: recall={}, precision={}\".format(recall_score(labels, prediction_default), precision_score(labels, prediction_default)))\n",
    "print(\"Updated Operating Point: recall={}, precision={}\".format(recall_score(labels, prediction_updated), precision_score(labels, prediction_updated)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy the selected candidate on an endpoint\n",
    "\n",
    "inference_response_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV\n",
    "\n",
    "predictor = automl.deploy(initial_instance_count=1,\n",
    "                          instance_type='ml.m5.xlarge',\n",
    "                          candidate=candidates[best_candidate_idx],\n",
    "                          inference_response_keys=inference_response_keys,\n",
    "                          predictor_cls=RealTimePredictor)\n",
    "predictor.content_type = CONTENT_TYPE_CSV\n",
    "\n",
    "print(\"Created endpoint: {}\".format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write test_data removing target 'is_defaulter'\n",
    "\n",
    "with open('test_data_no_target.csv', 'w') as csv_file:\n",
    "    test_data_no_target.to_csv(sep=',', header=False, index=False, path_or_buf=csv_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "prediction = predictor.predict(test_data_no_target.to_csv(sep=',', header=False, index=False)).decode('utf-8')\n",
    "prediction_df = pd.read_csv(StringIO(prediction), header=None, names=inference_response_keys)\n",
    "custom_predicted_labels = prediction_df.iloc[:,1].values >= best_candidate_threshold\n",
    "prediction_df['custom_predicted_label'] = custom_predicted_labels\n",
    "prediction_df['custom_predicted_label'] = prediction_df['custom_predicted_label'].map({False: target_attribute_values[0], True: target_attribute_values[1]})\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_defaulters = prediction_df[prediction_df[\"custom_predicted_label\"]==1]\n",
    "prediction_df_defaulters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_data['is_defaulter'], (predictions[best_candidate_idx].loc[:,0]>0.5))\n",
    "print('The Confusion Matrix is: \\n', cm)\n",
    "print('TrueNegative   FalsePositive\\nFalseNegative   TruePositive\\n')\n",
    "\n",
    "# Calculate the accuracy on test set\n",
    "predict_accuracy_on_test_set = (cm[0,0] + cm[1,1])/(cm[0,0] + cm[1,1]+cm[1,0] + cm[0,1])\n",
    "print('The Accuracy on Test Set is: ', predict_accuracy_on_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the AutoML provided a way better result for capturing those true positives, and the accuracy on the test set is also higher.\n",
    "\n",
    "Note Amazon SageMaker Autopilot works with SageMaker pipelines, where it chains different container images in sequence (e.g. Pre-processing container + XGBoost classifier container) for performing the inferences directly with the original data.\n",
    "\n",
    "In the next lab, we will learn how to industrialize/automate one of these examples following AWS MLOps best practices. We will choose the second case for simplicity and saving time, but note we could also use the AutoML artifacts from the final case if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "# s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# s3_bucket.objects.filter(Prefix=prefix).delete()\n",
    "\n",
    "# for transformer in transformers:\n",
    "#     transformer.delete_model()\n",
    "\n",
    "# predictor.delete_endpoint()\n",
    "# predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
