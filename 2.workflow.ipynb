{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab #2 - Build end-to-end ML workflow with AWS Step Functions Data Science SDK\n",
    "### Use Case: Industrializing a sample XGBoost model in AWS - Courier default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now assume that our model development work is ready, and we will start preparing the workflow for automating the training (and re-training) of the model with production data, and setup the bases for hosting this model in production later.\n",
    "\n",
    "-------\n",
    "\n",
    "**Courier default description:**\n",
    "\n",
    "*\"We define courier default when a courier collects a high amount of cash and then leaves the company without returning the money.\n",
    "\n",
    "- By leaving the company we define as not having any interaction with us during 28 consecutive days since the last delivered order.\n",
    "- Each region/city/country has a threshold to designate a high amount of cash defined by the business.\n",
    "\n",
    "The business request made to the Data Science team is to design a machine learning system to detect when a courier is likely to incur in default before they actually default or before the amount of cash balance is too high.\n",
    "\n",
    "The proposed definition is to deliver a default score for each courier every week. For that we created a label for each courier on a window time, so we can identify some defaulters and then fit a xgboost classifier to predict on current couriers.\"*\n",
    "\n",
    "-------\n",
    "\n",
    "***Please make sure you follow the steps for creating the AWS IAM roles required before running this notebook. These are used in the Amazon SageMaker notebook for interacting with other services like AWS Step Functions and AWS Lambda.***\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "Note the Data Scientists will use this notebook with Python 3 and the AWS Step Functions Data Science SDK installed for creating the ML workflow. We will also use AWS Lambda for some handy functions in the workflow.\n",
    "\n",
    "<img src=\"./sample_architecture.jpg\" width=\"60%\">\n",
    "\n",
    "*For more information on how to create a notebook or how to install the Data Science SDK you can check the documentation* [here](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/readmelink.html#getting-started-with-sample-jupyter-notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install/upgrade the stepfunctions library...\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### Preparation\n",
    "Let us start by loading some libraries and setting up the roles on the notebook.\n",
    "\n",
    "***--- NOTE: Make sure you replace with your bucket info and role ARNs in the placeholders below! ---***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, sagemaker, time, random, uuid, logging, stepfunctions, io, os, random\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep, TransformStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.template import TrainingPipeline\n",
    "from stepfunctions.template.utils import replace_parameters_with_jsonpath\n",
    "\n",
    "sagemaker_execution_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "###################################################\n",
    "###### REPLACE THE INFO IN THE PLACEHOLDERS #######\n",
    "###################################################\n",
    "# Replace with your AWS IAM ROLE ARN... in both places\n",
    "workflow_execution_role = 'REPLACE WITH YOUR ROLE ARN HERE'\n",
    "lambda_role = 'REPLACE WITH YOUR ROLE ARN HERE'\n",
    "# Replace with your Amazon S3 bucket name, prefix, and dataset filename...\n",
    "bucket = 'REPLACE WITH YOUR BUKCET NAME HERE'\n",
    "prefix = 'glovo/workflow'\n",
    "filename = 'joint_dataframe.csv'\n",
    "###################################################\n",
    "\n",
    "print('Using AWS StepFunctions and AWS Lambda roles:\\n {}\\n {}\\n'.format(workflow_execution_role, lambda_role))\n",
    "\n",
    "print('Uploading {} dataset to Amazon S3 at:\\n https://s3.console.aws.amazon.com/s3/buckets/{}/{}/'.format(filename, bucket, prefix))\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, filename)).upload_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "### Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating data preparation with AWS Lambda\n",
    "\n",
    "Prepare a script for running your sample feature engineering. We will use an AWS Lambda function for this because we want to automate as much as possible.\n",
    "\n",
    "Note we could also rely on Amazon SageMaker Processing for performing this data pre-processing. We will use a Lambda function in this case for simplicity and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data pre-processing AWS Lambda:\n",
    "\n",
    "# First download Pandas and Numpy for using in our AWS Lambda package, as these do not come in AWS Lambda's base...\n",
    "import os\n",
    "from urllib.request import urlretrieve \n",
    "!mkdir lambda\n",
    "urlretrieve(\"https://files.pythonhosted.org/packages/7b/fd/41698f20fd297cef2dc43a72a8ca42d149eaf7d954f1fb2bd3fc366a658d/pandas-0.25.3-cp38-cp38-manylinux1_x86_64.whl\", \"lambda/pandas-0.25.3-cp38-cp38-manylinux1_x86_64.whl\")\n",
    "urlretrieve(\"https://files.pythonhosted.org/packages/d7/6a/3fed132c846d1e47963f30376cc041e9dd586d286d931055ad06ff65c6c7/numpy-1.17.4-cp38-cp38-manylinux1_x86_64.whl\", \"lambda/numpy-1.17.4-cp38-cp38-manylinux1_x86_64.whl\")\n",
    "!unzip -o lambda/pandas-0.25.3-cp38-cp38-manylinux1_x86_64.whl -d lambda\n",
    "!unzip -o lambda/numpy-1.17.4-cp38-cp38-manylinux1_x86_64.whl -d lambda\n",
    "\n",
    "# then install the pytz dependency locally...\n",
    "!pip install -t lambda pytz\n",
    "# and remove the files no longer needed...\n",
    "!rm -rf lambda/*.whl lambda/*.dist-info lambda/__pycache__\n",
    "\n",
    "# prepare the Lambda function code for your pre-processing script...\n",
    "file_name = 'lambda/lambda_function.py'\n",
    "def MakeFile(file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write('''\\\n",
    "import json\n",
    "import boto3\n",
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "bucket = os.environ['BUCKET']\n",
    "prefix = os.environ['PREFIX']\n",
    "filename = os.environ['FILENAME']\n",
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket).download_file(prefix + \"/\" + filename, '/tmp/' + filename)\n",
    "    df = pandas.read_csv(\"/tmp/\" + filename)\n",
    "    cols = [0, 1, 2, 3, 4, 5, 6, 8, 56, 191, 201, 203, 206, 209, 218, 220]\n",
    "    df.drop(df.columns[cols],axis=1,inplace=True)\n",
    "    df = pandas.concat([df['is_defaulter'], df.drop(['is_defaulter'], axis=1)], axis=1)\n",
    "    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len(df)), int(0.9 * len(df))])\n",
    "    train_data.to_csv('/tmp/train.csv', header=False, index=False)\n",
    "    validation_data.to_csv('/tmp/validation.csv', header=False, index=False)\n",
    "    test_data.to_csv('/tmp/test_real.csv', header=False, index=False)\n",
    "    test_data.drop(['is_defaulter'], axis=1).to_csv('/tmp/test.csv', header=False, index=False)\n",
    "    s3.Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('/tmp/train.csv')\n",
    "    s3.Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('/tmp/validation.csv')\n",
    "    s3.Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('/tmp/test.csv')\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': ('Date pre-processing complete for ' + filename + ' - ' + bucket + '/' + prefix + '/' + filename)\n",
    "    }\n",
    "        ''')\n",
    "MakeFile(file_name)\n",
    "\n",
    "# finally, create the runtime file with Pandas, Numpy, and your pre-processing code...\n",
    "import zipfile\n",
    "def zipFilesInDir(dirName, zipFileName):\n",
    "   # create the zip file...\n",
    "   with zipfile.ZipFile(zipFileName, 'w', zipfile.ZIP_DEFLATED) as zipObj:\n",
    "        # iterate over all the files in the directory\n",
    "        #folder = os.path.abspath(dirName)\n",
    "        #os.chdir(folder)\n",
    "        for folderName, subfolders, filenames in os.walk(dirName):\n",
    "            for filename in filenames:\n",
    "               # create complete filepath of file in directory\n",
    "               filePath = os.path.join(folderName, filename)\n",
    "               # add file to zip\n",
    "               zipObj.write(filePath, arcname=os.path.join(os.path.relpath(folderName, os.path.abspath(dirName)), filename))\n",
    "zipFilesInDir('lambda', 'lambda_function.zip')\n",
    "!rm -fr lambda\n",
    "\n",
    "try:\n",
    "    f = open(\"lambda_function.zip\")\n",
    "    print(\"Lambda file created: lambda_function.zip\")\n",
    "except IOError:\n",
    "    print(\"Error - Lambda file not created\")\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# load the zip file as binary code...\n",
    "with open('lambda_function.zip', 'rb') as f: \n",
    "    code = f.read()\n",
    "\n",
    "# and create the AWS Lambda function...\n",
    "client = boto3.client('lambda')\n",
    "create_lambda_function = client.create_function(\n",
    "    FunctionName = 'glovo-pre-processing',\n",
    "    Runtime = 'python3.8',\n",
    "    Role = lambda_role,\n",
    "    Description = 'Sample ML pipeline data pre-processing lambda',\n",
    "    Code = {'ZipFile': code},\n",
    "    Handler='{}.lambda_handler'.format('lambda_function'),\n",
    "    Timeout = 600,\n",
    "    MemorySize = 1024,\n",
    "    Environment = {\n",
    "        'Variables': {\n",
    "            'BUCKET': bucket,\n",
    "            'PREFIX': prefix,\n",
    "            'FILENAME': filename\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "### Building our ML pipeline with the AWS Step Functions Data Science SDK\n",
    "\n",
    "You are now ready for creating your actual ML pipeline steps. You will start by preparing the training job for Amazon SageMaker, and the data transformation function in AWS Lambda that you created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(\n",
    "    get_image_uri(boto3.Session().region_name, 'xgboost'),\n",
    "    sagemaker_execution_role, \n",
    "    train_instance_count = 1, \n",
    "    train_instance_type = 'ml.m5.xlarge',\n",
    "    train_volume_size = 5,\n",
    "    output_path = 's3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session = session\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(objective='binary:logistic',\n",
    "                         alpha = 0.0017958870152480393,\n",
    "                         colsample_bytree = 0.8974444697232986,\n",
    "                         eta = 0.378416419404957,\n",
    "                         gamma = 0.0038479366336815115,\n",
    "                         max_depth = 22,\n",
    "                         min_child_weight = 3.4445863514152535,\n",
    "                         num_round = 139,\n",
    "                         subsample = 0.7432022124726009\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for jobs/models/endpoints. Pass these for each execution via placeholders:\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'JobName': str, \n",
    "    'ModelName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_step = steps.LambdaStep(\n",
    "    'Preparing data (Lambda)',\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"glovo-pre-processing\",\n",
    "        \"Payload\": {  \n",
    "           \"JobName\": execution_input['JobName']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "preparation_step.add_retry(steps.Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=15,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=4.0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    'Training (SageMaker)', \n",
    "    estimator=xgb,\n",
    "    data={\n",
    "        'train': sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv'),\n",
    "        'validation': sagemaker.s3_input(s3_data='s3://{}/{}/validation'.format(bucket, prefix), content_type='csv')\n",
    "    },\n",
    "    job_name=execution_input['JobName']  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save model (SageMaker)',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName']  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now add another AWS Lambda function for validating the accuracy of your trained model, once the training job completes. In this case, use the metric for the validation loss provided by the Amazon SageMaker XGBoost model by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation AWS Lambda:\n",
    "file_name = 'lambda_function.py'\n",
    "def MakeFile(file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write('''\\\n",
    "import json\n",
    "import boto3\n",
    "def lambda_handler(event, context):\n",
    "    sm = boto3.client('sagemaker')\n",
    "    vloss = sm.describe_training_job(TrainingJobName=event['JobName'])['FinalMetricDataList'][0]['Value']\n",
    "    print(vloss)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'vloss': json.dumps(vloss)\n",
    "    }\n",
    "        ''')\n",
    "MakeFile(file_name)\n",
    "\n",
    "# create the zip file...\n",
    "with zipfile.ZipFile('lambda_function.zip', 'w', zipfile.ZIP_DEFLATED) as zipObj:\n",
    "    zipObj.write('lambda_function.py')\n",
    "!rm -f lambda_function.py\n",
    "\n",
    "try:\n",
    "    f = open(\"lambda_function.zip\")\n",
    "    print(\"Lambda file created: lambda_function.zip\")\n",
    "except IOError:\n",
    "    print(\"Error - Lambda file not created\")\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Loads the zip file as binary code. \n",
    "with open('lambda_function.zip', 'rb') as f: \n",
    "    code = f.read()\n",
    "\n",
    "client = boto3.client('lambda')\n",
    "create_lambda_function = client.create_function(\n",
    "    FunctionName = 'glovo-validation',\n",
    "    Runtime = 'python3.8',\n",
    "    Role = lambda_role,\n",
    "    Handler = '{}.lambda_handler'.format('lambda_function'),\n",
    "    Description = 'Sample ML pipeline validation metric lambda',\n",
    "    Code = {'ZipFile': code},\n",
    "    Timeout = 600,\n",
    "    MemorySize = 128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_lambda_step = steps.LambdaStep(\n",
    "    'Validating loss (Lambda)',\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"glovo-validation\",\n",
    "        \"Payload\": {  \n",
    "           \"JobName\": execution_input['JobName']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "validation_lambda_step.add_retry(steps.Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=15,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=4.0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrating a typical decision logic that is common in ML workflows, you might want to automate the process of deploying new models to production when new data is available and re-trainings are performed. For example, by comparing the new models’ performance versus a given threshold.\n",
    "\n",
    "In this example, you decide upon the validation loss whether to continue the ML pipeline for running inferences for the new model in production, or otherwise stop the pipeline if the new model is not meeting the accuracy threshold (set at 3% just for illustrating the concept). This is a mechanism that looks to automate the re-training with new data. For example, you could trigger this ML pipeline everyday with the new data provided from your recent customer's transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_step = steps.TransformStep(\n",
    "    'Batch inference (SageMaker)',\n",
    "    transformer=xgb.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.large'\n",
    "    ),\n",
    "    job_name=execution_input['JobName'],     \n",
    "    model_name=execution_input['ModelName'], \n",
    "    data='s3://{}/{}/test'.format(bucket, prefix),\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_step = steps.Pass(\n",
    "    'Worse model',\n",
    "    parameters={\n",
    "        \"Error\": (\"The new model is not accurate enough. Validation loss:\" + str(validation_lambda_step.output()[\"Payload\"][\"vloss\"]))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_state = steps.Choice(\n",
    "    state_id='validation loss < 3% ?' #your desired threshold for validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss values with your desired threshold\n",
    "choice_state.add_choice(\n",
    "    rule=steps.ChoiceRule.StringLessThan(variable=validation_lambda_step.output()[\"Payload\"][\"vloss\"], value=\"0.03\"),\n",
    "    next_step=transform_step\n",
    ")\n",
    "choice_state.add_choice(\n",
    "    rule=steps.ChoiceRule.StringGreaterThanEquals(variable=validation_lambda_step.output()[\"Payload\"][\"vloss\"], value=\"0.03\"),\n",
    "    next_step=worse_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the validation loss is good enough, you can now perform an Amazon SageMaker Batch Transformation for running inferences on all of your testing dataset.\n",
    "\n",
    "*Note just FYI: In this case you are not creating and 'Endpoint Configuration' and an 'Endpoint', but shall you need it in your use case to respond to real-time inferences you can then follow the steps in the AWS Step Functions Data Science SDK examples, similar to the following lines:*\n",
    "```\n",
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "endpoint_step = steps.EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready for chaining the steps of your ML pipeline with the AWS Step Functions Data Science SDK, and set up the workflow with the create command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    preparation_step,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    validation_lambda_step,\n",
    "    choice_state\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(\n",
    "    name='glovo-courier-notebook',\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph(portrait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that when you use the SDK in Jupyter notebooks, you define the workflows locally in the notebook instance but **they do not actually exist on AWS Step Functions until the “create” command is called**. Similarly, **these are not executed until the “execute” command is called**, after which you can track its progress on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready for testing this pipeline by calling the execute command, and debugging its execution logs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'JobName': 'glovo-courier-default-{}'.format(uuid.uuid1().hex), # Each Sagemaker Job requires a unique name\n",
    "        'ModelName': 'glovo-courier-default-{}'.format(uuid.uuid1().hex), # Each Model requires a unique name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note you can re-run the previous cell until verifying the execution is completed. It is also possible to verify the execution details directly on the [AWS Step Functions console](https://eu-west-1.console.aws.amazon.com/states/).*\n",
    "\n",
    "You can also list the events with its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_events(html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.list_executions(html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.list_workflows(html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even export the AWS CloudFormation template for the pipeline you have just built, in order to deploy it later on as infrastructure as code if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Workflow.get_cloudformation_template(workflow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, your ML workflow is now complete, and the data scientists can now pass this Cloud Formation template to the DevOps engineers for setting up the full piipeline (see the next lab).\n",
    "\n",
    "As a bonus, you can follow the steps in this documentation for setting up an automatic triggering of our workflow as soon as our Amazon S3 bucket receives new data, in order to automate the re-training of the model.\n",
    "https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
